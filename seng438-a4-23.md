SENG 438 - Software Testing, Reliability, and Quality

Lab Report #4 - Mutation and GUI Testing

| Group :      |  23  |
| -------------- | --- |
| Student Names: |     |
| M Munem Morhsed |     |
| Himel Paul |     |
| SM Wahid Chowdhury |     |
| Maliha Chowdhury Adrita |     |
| Kosy Onyejemezi |     |

# 1 Introduction

This lab report presents our work on two major aspects of software testing: Mutation Testing and GUI Testing. In Part 1, we applied mutation testing to evaluate the effectiveness of our test suites using Pitest on the JFreeChart codebase. In Part 2, we automated GUI tests using Selenium IDE for selected functionalities on a chosen website and compared Selenium with Sikulix.

# Analysis of 10 Mutants of the Range class 

| Mutant Number | Mutation Line Occurrence in Range.java | Mutation Description                                     | Status of Mutant |
|---------------|-----------------------------------------|------------------------------------------------------------------------|------------------|
| 1             | Line 305                                | Changed conditional boundary                                           | SURVIVED         |
| 2             | Line 306                                | Negated conditional                                                    | KILLED           |
| 3             | Line 308                                | Replaced return value with null for `expandToInclude`                 | KILLED           |
| 4             | Line 312                                | Replaced return value with null for `expandToInclude`                 | KILLED           |
| 5             | Line 329                                | Removed call to `ParamChecks::nullNotPermitted`                       | SURVIVED         |
| 6             | Line 331                                | Replaced double multiplication with division                           | KILLED           |
| 7             | Line 332                                | Replaced double addition with subtraction                              | KILLED           |
| 8             | Line 333                                | Changed conditional boundary                                           | SURVIVED         |
| 9             | Line 334                                | Replaced double division with multiplication                           | KILLED           |
| 10            | Line 349                                | Replaced return value with null for `expand`                           | KILLED           |


# All the statistics and the mutation score for each test classs

Before: 

![](https://github.com/malihachowdhury6/SENG438_A4/blob/main/images/before.jpeg?raw=true)

After:
![](https://github.com/malihachowdhury6/SENG438_A4/blob/main/images/after.jpeg?raw=true)

# A discussion on the effect of equivalent mutants on mutation score accuracy 

Equivalent mutants negatively affect the accuracy of mutation scores because they cannot be killed by any test case, even if the test suite is comprehensive. As a result, they falsely lower the mutation score, making the test suite appear weaker than it actually is. This can lead to misleading conclusions about test effectiveness. Detecting equivalent mutants is challenging because it requires identifying whether the mutant's behavior is truly indistinguishable from the original code. One approach is manual inspection of the mutation logs and analyzing the corresponding code to spot changes that do not affect program logic—for example, syntactic variations or redundant conditions. Static code analysis tools and symbolic execution techniques may also help identify some equivalent mutants automatically. However, most detection methods are time-consuming or only partially effective, highlighting a limitation of mutation testing tools.

# A discussion of how the mutation score improved

To improve the mutation scores for both Range and DataUtilities, we analyzed the surviving mutants generated by Pitest and identified untested conditions, edge cases, and boundary values. In particular, we noticed that some mutants survived due to missing test coverage for null inputs, extreme values, and certain combinations of method parameters. Based on this analysis, we wrote additional test cases that explicitly targeted these untested paths and behaviors. For example, in the RangeTest, we added tests for overlapping and non-overlapping range combinations, and in DataUtilitiesTest, we included cases with empty arrays, negative numbers, and null references. These additions allowed us to increase the mutation score by covering logical branches and code paths that were previously untested, leading to a measurable improvement in test suite quality.

# A discussion on the advantages and disadvantages of mutation testing

Mutation testing provides a deeper measure of test suite effectiveness by introducing small faults (mutants) into the code and checking whether existing tests can detect them. One major advantage is that it uncovers inadequacies in the test suite that traditional code coverage metrics might miss. It encourages writing more robust tests by exposing edge cases and subtle logic flaws. Additionally, it can reveal unreachable or untested parts of the code, helping developers improve test coverage in a meaningful way. However, mutation testing also has several drawbacks. It can be computationally expensive, especially for large codebases, as it involves running tests against many mutated versions of the code. The process can be time-consuming and may produce a large number of mutants, including equivalent ones that cannot be killed, which can skew mutation scores and require manual investigation. Despite its limitations, mutation testing remains a powerful technique for improving test suite quality when used thoughtfully.

# GUI Testing
# Correctness and completeness of the Recorded test cases (at least 2 per group member)

Each group member recorded at least two Selenium test cases, resulting in a total of ten tests, which are all included in the submitted selenium folder as .side files. These tests cover a variety of functionalities such as applying product filters, changing store locations, adding items to the cart, and proceeding to checkout. We ensured that each script ran successfully and completed without errors, verifying that all intended interactions were correctly captured and executed. The tests reflected realistic user behavior and were designed to validate core features of the selected websites. All recorded test cases passed during execution

# Explain your test case design process

Our test case design process focused on identifying common and critical user interactions for each selected website. We began by exploring the core functionalities such as searching for products, adding items to the cart, applying filters, changing store locations, and navigating to checkout. For each functionality, we defined both typical (positive) and edge (negative) scenarios—for example, adding an item to the cart versus attempting to proceed to checkout with an empty cart. These scenarios were then translated into step-by-step actions using Selenium IDE, ensuring that verification points were added to confirm expected outcomes. We also varied the input data across test cases to simulate realistic user behavior. This approach ensured our tests were comprehensive, reusable, and effective in validating the core features of each site.


# Use of automated verification points in each script

In each Selenium test script, we incorporated automated verification points to confirm that the expected outcomes were achieved after each interaction. For example, after adding an item to the cart, we verified that the cart count or subtotal was updated accordingly. When performing a store location change on Home Depot, we checked that the displayed store name reflected the new selection. Similarly, for product searches, we validated that the results page contained relevant items or applied filters. These automated assertions allowed our tests to detect failures or unexpected behavior immediately, making the scripts more robust and reliable. 

# Use different test data per test

To ensure thorough coverage and realistic simulation of user behavior, we used different test data and performed varied tasks across the assigned websites: Home Depot, Gap Canada, and Real Canadian Superstore. For Home Depot, we designed multiple test cases including adding an item to the cart and proceeding to checkout, changing the store location, and applying filters to search for specific products. On the Gap Canada website, we tested adding items to the cart and then removing them to validate cart update functionality. Each test involved different user actions and data inputs, such as product names, filters, or locations, allowing us to effectively evaluate a wide range of interactions and UI behaviors.

# Discuss advantages and disadvantages of Selenium vs. Sikulix

Selenium and Sikulix are both automation tools but serve different purposes and have distinct strengths. Selenium is specifically designed for web application testing and interacts directly with HTML elements, making it reliable and efficient for validating web-based workflows. It supports automated verification, cross-browser testing, and integrates well with modern CI/CD pipelines. However, it is limited to web environments and can become fragile when dealing with dynamic or complex page structures. On the other hand, Sikulix uses image recognition to automate any GUI, whether web or desktop, by visually identifying elements on the screen. This makes it suitable for applications that Selenium cannot test, such as legacy software or graphical interfaces without accessible HTML. Despite its flexibility, Sikulix is more sensitive to screen resolution and layout changes, which can lead to brittle tests. It also lacks advanced verification features and is generally slower and harder to maintain for large-scale automation. Overall, Selenium is the better choice for structured, web-based applications, while Sikulix offers versatility at the cost of robustness and maintainability.

# A discussion on how the team work/effort was divided and managed. Any lessons learned from your teamwork on this lab?

Our team of five divided the workload evenly to ensure efficiency and collaboration throughout the assignment. Each member was responsible for designing and automating two Selenium test cases, covering different functionalities across the provided websites to maximize coverage. Additionally, each member contributed two JUnit test cases from their own previous work for mutation testing. To improve the mutation scores, three team members focused on enhancing the DataUtilitiesTest suite by identifying gaps and adding targeted test cases, while the remaining two worked on refining the RangeTest suite. This clear division of responsibilities helped us stay organized and meet all requirements effectively. One key lesson we learned was the importance of early coordination and regularly syncing our progress. This helped avoid overlapping efforts, ensured consistent test coverage, and made integration smoother as we prepared for the demo and final submission.
