SENG 438 - Software Testing, Reliability, and Quality

Lab Report #4 - Mutation and GUI Testing

| Group :      |  23  |
| -------------- | --- |
| Student Names: |     |
| M Munem Morhsed |     |
| Himel Paul |     |
| SM Wahid Chowdhury |     |
| Maliha Chowdhury Adrita |     |
| Kosy Onyejemezi |     |

# 1 Introduction

This lab report presents our work on two major aspects of software testing: Mutation Testing and GUI Testing. In Part 1, we applied mutation testing to evaluate the effectiveness of our test suites using Pitest on the JFreeChart codebase. In Part 2, we automated GUI tests using Selenium IDE for selected functionalities on a chosen website and compared Selenium with Sikulix.

# Analysis of 10 Mutants of the Range class 

| Mutant Number | Mutation Line Occurrence in Range.java | Mutation Description                                     | Status of Mutant |
|---------------|-----------------------------------------|------------------------------------------------------------------------|------------------|
| 1             | Line 305                                | Changed conditional boundary                                           | SURVIVED         |
| 2             | Line 306                                | Negated conditional                                                    | KILLED           |
| 3             | Line 308                                | Replaced return value with null for `expandToInclude`                 | KILLED           |
| 4             | Line 312                                | Replaced return value with null for `expandToInclude`                 | KILLED           |
| 5             | Line 329                                | Removed call to `ParamChecks::nullNotPermitted`                       | SURVIVED         |
| 6             | Line 331                                | Replaced double multiplication with division                           | KILLED           |
| 7             | Line 332                                | Replaced double addition with subtraction                              | KILLED           |
| 8             | Line 333                                | Changed conditional boundary                                           | SURVIVED         |
| 9             | Line 334                                | Replaced double division with multiplication                           | KILLED           |
| 10            | Line 349                                | Replaced return value with null for `expand`                           | KILLED           |


# All the statistics and the mutation score for each test classs

Before: 

![](https://github.com/malihachowdhury6/SENG438_A4/blob/main/images/before.jpeg?raw=true)

After:
![](https://github.com/malihachowdhury6/SENG438_A4/blob/main/images/after.jpeg?raw=true)

# A discussion on the effect of equivalent mutants on mutation score accuracy 

Equivalent mutants are those that, despite being syntactically different, behave exactly like the original code and cannot be killed by any test case. Their presence lowers the mutation score unfairly, making the test suite appear less effective. This affects the accuracy of mutation testing as a metric. Detecting equivalent mutants is difficult and often requires manual analysis or advanced static code analysis to identify changes that donâ€™t impact program behavior. In our case, we identified some equivalents by comparing outputs and confirming no behavioral differences. While useful, this process is time-consuming and highlights the need for better automated detection methods in mutation testing tools.

# A discussion of how the mutation score improved

To improve the mutation scores for both Range and DataUtilities, we analyzed the surviving mutants generated by Pitest and identified untested conditions, edge cases, and boundary values. In particular, we noticed that some mutants survived due to missing test coverage for null inputs, extreme values, and certain combinations of method parameters. Based on this analysis, we wrote additional test cases that explicitly targeted these untested paths and behaviors. For example, in the RangeTest, we added tests for overlapping and non-overlapping range combinations, and in DataUtilitiesTest, we included cases with empty arrays, negative numbers, and null references. These additions allowed us to increase the mutation score by covering logical branches and code paths that were previously untested, leading to a measurable improvement in test suite quality.

# A discussion on the advantages and disadvantages of mutation testing

Mutation testing provides a deeper measure of test suite effectiveness by introducing small faults (mutants) into the code and checking whether existing tests can detect them. One major advantage is that it uncovers inadequacies in the test suite that traditional code coverage metrics might miss. It encourages writing more robust tests by exposing edge cases and subtle logic flaws. Additionally, it can reveal unreachable or untested parts of the code, helping developers improve test coverage in a meaningful way. However, mutation testing also has several drawbacks. It can be computationally expensive, especially for large codebases, as it involves running tests against many mutated versions of the code. The process can be time-consuming and may produce a large number of mutants, including equivalent ones that cannot be killed, which can skew mutation scores and require manual investigation. Despite its limitations, mutation testing remains a powerful technique for improving test suite quality when used thoughtfully.

# GUI Testing

# Correctness and completeness of the Recorded test cases (at least 2 per group member)

# Explain your test case design process

# Use of automated verification points in each script

# Use different test data per test

To ensure thorough coverage and realistic simulation of user behavior, we used different test data and performed varied tasks across the assigned websites: Home Depot, Gap Canada, and Real Canadian Superstore. For Home Depot, we designed multiple test cases including adding an item to the cart and proceeding to checkout, changing the store location, and applying filters to search for specific products. On the Gap Canada website, we tested adding items to the cart and then removing them to validate cart update functionality. Each test involved different user actions and data inputs, such as product names, filters, or locations, allowing us to effectively evaluate a wide range of interactions and UI behaviors.

# Discuss advantages and disadvantages of Selenium vs. Sikulix

Selenium and Sikulix are both automation tools but serve different purposes and have distinct strengths. Selenium is specifically designed for web application testing and interacts directly with HTML elements, making it reliable and efficient for validating web-based workflows. It supports automated verification, cross-browser testing, and integrates well with modern CI/CD pipelines. However, it is limited to web environments and can become fragile when dealing with dynamic or complex page structures. On the other hand, Sikulix uses image recognition to automate any GUI, whether web or desktop, by visually identifying elements on the screen. This makes it suitable for applications that Selenium cannot test, such as legacy software or graphical interfaces without accessible HTML. Despite its flexibility, Sikulix is more sensitive to screen resolution and layout changes, which can lead to brittle tests. It also lacks advanced verification features and is generally slower and harder to maintain for large-scale automation. Overall, Selenium is the better choice for structured, web-based applications, while Sikulix offers versatility at the cost of robustness and maintainability.

# A discussion on how the team work/effort was divided and managed. Any lessons learned from your teamwork on this lab?
